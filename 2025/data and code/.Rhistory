main = "Power Rates",
ylim = c(0, 1))
## test boot function
## generate data
initial_time = data_generation(N = N, J = J, N_dif = N_dif, tau, beta, alpha)
dif_matrix = dif_applier(initial_time, N_dif, 1,
tau, alpha, beta, 0.2)
tau_hat = tau_estimation(dif_matrix, N)
test_boot = bootstrap_lasso_significance(X = cbind(tau_hat, G), y = dif_matrix[,1])
test_boot$p_values[2]
### DIF lasso implementation for uniform DIF
setwd(dirname(rstudioapi::documentPath()))
library(glmnet)
source('dif lasso functions.R')
set.seed(1111)
# conditions
N = 500
N_dif = 100
J = 20
# Item and person parameters
tau = round(rnorm(N, 0, 0.3), 3)
beta = round(rnorm(J, 4, 0.45), 3)
alpha = round(rnorm(J, 1.85, 0.15), 3)
delta_beta = c(0, 0.1, 0.2, 0.3, 0.4, 0.5)
G = c(rep(1,N_dif), rep(0,N - N_dif))
#### start here:
power_rates = c()  # To store power rates for each delta_beta
for(b in 1:length(delta_beta)) {
delta_b = delta_beta[b]
significant_count = 0  # Count how many times we detect significant DIF
for(i in 1:100) {  # Run 100 iterations for each delta_beta
## generate data
initial_time = data_generation(N = N, J = J, N_dif = N_dif, tau, beta, alpha)
dif_matrix = dif_applier(initial_time, N_dif, 1, tau, alpha, beta, delta_b)
tau_hat = tau_estimation(dif_matrix, N)
## Run permutation test
test_perm = lasso_permutation_test(X = cbind(tau_hat, G), y = dif_matrix[,1])
# Check if beta2 (group effect) is significant using permutation test
if(test_perm$p_values[2] < 0.05) {  # Using 0.05 significance level
significant_count = significant_count + 1
}
}
# Calculate power rate (proportion of times we detected significant DIF)
power_rate = significant_count / 100
power_rates = c(power_rates, power_rate)
}
# Create a plot of power rates
plot(delta_beta,
power_rates,
type = "o",  # Points and lines
col = "red",
lwd = 2,     # Line width
pch = 16,    # Point type
xlab = "Delta Beta",
ylab = "Power Rates",
main = "Power Rates (Permutation Test Method)",
ylim = c(0, 1))
# Print the power rates
print("Power rates for each delta_beta value:")
print(data.frame(delta_beta = delta_beta, power_rate = power_rates))
test_perm
coef(test_perm)
p_values
test_perm$p_values
# Create a plot of power rates
plot(delta_beta,
power_rates,
type = "o",  # Points and lines
col = "red",
lwd = 2,     # Line width
pch = 16,    # Point type
xlab = "Delta Beta",
ylab = "Power Rates (with 0 being type I error rate)",
main = "Power Rates & Type I error rate (permutation test)",
ylim = c(0, 1))
# Print the power rates
print("Power rates for each delta_beta value:")
print(data.frame(delta_beta = delta_beta, power_rate = power_rates))
seq(0, 0.4, 0.05)
### DIF lasso implementation for uniform DIF
setwd(dirname(rstudioapi::documentPath()))
library(glmnet)
source('dif lasso functions.R')
set.seed(1111)
# conditions
N = 500
N_dif = 100
J = 20
# Item and person parameters
tau = round(rnorm(N, 0, 0.3), 3)
beta = round(rnorm(J, 4, 0.45), 3)
alpha = round(rnorm(J, 1.85, 0.15), 3)
delta_beta = seq(0, 0.4, 0.05)
G = c(rep(1,N_dif), rep(0,N - N_dif))
#### start here:
power_rates = c()  # To store power rates for each delta_beta
for(b in 1:length(delta_beta)) {
delta_b = delta_beta[b]
significant_count = 0  # Count how many times we detect significant DIF
for(i in 1:100) {  # Run 100 iterations for each delta_beta
## generate data
initial_time = data_generation(N = N, J = J, N_dif = N_dif, tau, beta, alpha)
dif_matrix = dif_applier(initial_time, N_dif, 1, tau, alpha, beta, delta_b)
tau_hat = tau_estimation(dif_matrix, N)
## Run permutation test
test_perm = lasso_permutation_test(X = cbind(tau_hat, G), y = dif_matrix[,1])
# Check if beta2 (group effect) is significant using permutation test
if(test_perm$p_values[2] < 0.05) {  # Using 0.05 significance level
significant_count = significant_count + 1
}
}
# Calculate power rate (proportion of times we detected significant DIF)
power_rate = significant_count / 100
power_rates = c(power_rates, power_rate)
}
# Create a plot of power rates
plot(delta_beta,
power_rates,
type = "o",  # Points and lines
col = "red",
lwd = 2,     # Line width
pch = 16,    # Point type
xlab = "Delta Beta",
ylab = "Power Rates (with 0 being type I error rate)",
main = "Power Rates & Type I error rate (permutation test)",
ylim = c(0, 1))
# Print power rates
print("Power rates for each delta_beta value:")
print(data.frame(delta_beta = delta_beta, power_rate = power_rates))
### Type I Error Analysis for DIF Detection
setwd(dirname(rstudioapi::documentPath()))
### Type I Error Analysis for DIF Detection
# setwd(dirname(rstudioapi::documentPath()))
library(glmnet)
source('dif lasso functions.R')
# Set seed for reproducibility
set.seed(1111)
# Simulation parameters
N = 500
N_dif = 100
J = 20
n_sims = 1000  # Number of simulation iterations
# Initialize parameters
tau = round(rnorm(N, 0, 0.3), 3)
beta = round(rnorm(J, 4, 0.45), 3)
alpha = round(rnorm(J, 1.85, 0.15), 3)
G = c(rep(1,N_dif), rep(0,N - N_dif))
# Storage for results
rejections = 0  # Count rejections (p < 0.05)
p_values = numeric(n_sims)  # Store all p-values
# Run simulations with delta_beta = 0 (no DIF)
for(i in 1:n_sims) {
# Generate data with no DIF effect
initial_time = data_generation(N = N, J = J, N_dif = N_dif, tau, beta, alpha)
dif_matrix = dif_applier(initial_time, N_dif, 1, tau, alpha, beta, delta_beta = 0)
tau_hat = tau_estimation(dif_matrix, N)
# Run permutation test
test_perm = lasso_permutation_test(X = cbind(tau_hat, G), y = dif_matrix[,1])
# Store p-value for group effect (beta2)
p_values[i] = test_perm$p_values[2]
# Count rejection if p < 0.05
if(p_values[i] < 0.05) {
rejections = rejections + 1
}
# Print progress every 100 iterations
if(i %% 100 == 0) {
print(paste("Completed iteration", i))
}
}
# Calculate Type I error rate
type1_error = rejections / n_sims
# Results
print(paste("Type I Error Rate:", round(type1_error, 3)))
# Plot distribution of p-values
hist(p_values,
breaks = 20,
main = "Distribution of P-values Under H0",
xlab = "P-value",
freq = FALSE)
abline(h = 1, col = "red", lty = 2)  # Reference line for uniform distribution
# Save results
results = list(
type1_error = type1_error,
p_values = p_values
)
print(results)
# Save results
type1_error
818 * 3
7285.53 + 3712.47 + 1443.78 + 3492.58
730 + 751 + 555 + 579 + 739 + 1070
4424 + 7285.53 + 3712.47 + 1443.78 + 3492.58
4398.51 + 2809.12 + 2315.89 + 2938.91 + 2240.80 + 4218.50
18921.73 - 20358.36
23350.19 + 7285.53 + 3712.47 + 1443.78 + 3492.58
1150+30+35+70+200
1485 * 12
17820 + 39284.55
75000 - 57104.55
500 + 300
800 * 12
17820 - 9600
12+4+5
5000 * 6
20*12
### DIF lasso implementation for uniform DIF
setwd(dirname(rstudioapi::documentPath()))
library(glmnet)
source('dif lasso functions.R')
set.seed(1111)
# conditions
N = 500
N_dif = 100
J = 20
# item and person parameters
tau = round(rnorm(N, 0, 0.3), 3)
beta = round(rnorm(J, 4, 0.45), 3)
alpha = round(rnorm(J, 1.85, 0.15), 3)
delta_beta = seq(0, 0.4, 0.05)
G = c(rep(1,N_dif), rep(0,N - N_dif))
#### start here:
power_rates = c()  #store power rates for each delta_beta
for(b in 1:length(delta_beta)) {
delta_b = delta_beta[b]
significant_count = 0  # count how many times we detect significant DIF
for(i in 1:100) {  # 100 iterations for each delta_beta
## generate data
initial_time = data_generation(N = N, J = J, N_dif = N_dif, tau, beta, alpha)
dif_matrix = dif_applier(initial_time, N_dif, 1, tau, alpha, beta, delta_b)
tau_hat = tau_estimation(dif_matrix, N)
## permutation test
test_perm = lasso_permutation_test(X = cbind(tau_hat, G), y = dif_matrix[,1])
# check if beta2 (group effect) is significant using permutation test
if(test_perm$p_values[2] < 0.05) {  # Using 0.05 significance level
significant_count = significant_count + 1
}
}
# calculate power rate (proportion of times we detected significant DIF)
power_rate = significant_count / 100
power_rates = c(power_rates, power_rate)
}
# plot the rates
plot(delta_beta,
power_rates,
type = "o",  # Points and lines
col = "red",
lwd = 2,     # Line width
pch = 16,    # Point type
xlab = "Delta Beta",
ylab = "Power Rates (with 0 being type I error rate)",
main = "Power Rates & Type I error rate (permutation test)",
ylim = c(0, 1))
# type I error rate and power rates
print("Power rates for each delta_beta value:")
print(data.frame(delta_beta = delta_beta, power_rate = power_rates))
27* 0.2
11000 + 544 * 12
1500 * 12
11000 + 544 * 12 + 200* 12
11000 + 475 * 12 + 200* 12
11000 + 475 * 12 * 2 + 200* 12 * 2
18000 * 2
36000 - 27200
11000 + 475 * 12 * 3 + 200* 12 * 3
1500 * 12 * 3
54000 - 35300
###############################
### ptg main analysis script
###############################
library(tidyverse)
library(readxl)
library(metafor)
library(psych)
library(kableExtra)
library(modelsummary)
setwd(dirname(rstudioapi::documentPath()))
## load data
shortlist = read_excel('effect_sizes_and_moderators.xlsx')
shortlist %>% group_by(`scale type`) %>% count() # 20 PTGI and 10 PTGI-SF
## we can do normalization or we can perform separate analysis for these two types of studies
## check the sample size
sum(shortlist$`sample size`) ## overall
PTGI_dat = shortlist %>% filter(`scale type` == 'PTGI')
# PTGISF_dat = shortlist %>% filter(`scale type` != 'PTGI')
#
# ## use escalc function to get effect sizes
calculate_effect_size = escalc(measure = "SMD",
m1i = PTGI_dat$`effect size`,
sd1i = PTGI_dat$sd,
n1i = PTGI_dat$`sample size`,  # Use actual sample sizes
m2i = rnorm(nrow(PTGI_dat), mean = 45, sd = PTGI_dat$sd),
sd2i = PTGI_dat$sd,
n2i = PTGI_dat$`sample size`,  # Use actual sample sizes
data = PTGI_dat,
append = TRUE)
calculate_effect_size ## noticed some quite high values;
# this indicates that maybe screening for outlier is needed
## we run intercept only model for main analysis (this would be random intercept model)
main_analysis_model_PTGI = rma(yi, vi, data = calculate_effect_size)
main_analysis_model_PTGI
# forest plot
forest(main_analysis_model_PTGI, slab = `Source`, header = "Study")
# influence
influence(main_analysis_model_PTGI)
## moderators
mod.groups = rma(yi, vi, mods = ~ factor(Groups), data = calculate_effect_size)
mod.groups
##############################################
##############################################
## examine PTGI data
high_exposure_group = PTGI_dat$Groups == "Front Line worker" |
PTGI_dat$Groups == "Nurses" |
PTGI_dat$Groups == "Medical Doctors" |
PTGI_dat$Groups == "Health Care Workers" |
PTGI_dat$Groups == "Patients"
first.half <- data.frame(PTGI_dat[high_exposure_group,
-which(names(PTGI_dat) %in%
c("Groups", "Mean Age" ,
"scale type",
"female proportion(Marg)"))])
second.half <- data.frame(PTGI_dat[!high_exposure_group,
-which(names(PTGI_dat) %in%
c("Groups", "Mean Age" ,
"scale type",
"female proportion(Marg)"))])
new_dat <- cbind(rbind(first.half, second.half),
exposure = ifelse(high_exposure_group == TRUE, 1, 0))
###############################
### ptg main analysis script
###############################
library(tidyverse)
library(readxl)
library(metafor)
library(psych)
library(kableExtra)
library(modelsummary)
setwd(dirname(rstudioapi::documentPath()))
## load data
shortlist = read_excel('effect_sizes_and_moderators.xlsx')
shortlist %>% group_by(`scale type`) %>% count() # 20 PTGI and 10 PTGI-SF
## we can do normalization or we can perform separate analysis for these two types of studies
## check the sample size
sum(shortlist$`sample size`) ## overall
PTGI_dat = shortlist %>% filter(`scale type` == 'PTGI')
# PTGISF_dat = shortlist %>% filter(`scale type` != 'PTGI')
#
# ## use escalc function to get effect sizes
calculate_effect_size = escalc(measure = "SMD",
m1i = PTGI_dat$`effect size`,
sd1i = PTGI_dat$sd,
n1i = PTGI_dat$`sample size`,  # Use actual sample sizes
m2i = rnorm(nrow(PTGI_dat), mean = 45, sd = PTGI_dat$sd),
sd2i = PTGI_dat$sd,
n2i = PTGI_dat$`sample size`,  # Use actual sample sizes
data = PTGI_dat,
append = TRUE)
calculate_effect_size ## noticed some quite high values;
# this indicates that maybe screening for outlier is needed
## we run intercept only model for main analysis (this would be random intercept model)
main_analysis_model_PTGI = rma(yi, vi, data = calculate_effect_size)
main_analysis_model_PTGI
# forest plot
forest(main_analysis_model_PTGI, slab = `Source`, header = "Study")
# influence
influence(main_analysis_model_PTGI)
## moderators
mod.groups = rma(yi, vi, mods = ~ factor(Groups), data = calculate_effect_size)
mod.groups
##############################################
##############################################
## examine PTGI data
high_exposure_group = PTGI_dat$Groups == "Front Line worker" |
PTGI_dat$Groups == "Nurses" |
PTGI_dat$Groups == "Medical Doctors" |
PTGI_dat$Groups == "Health Care Workers" |
PTGI_dat$Groups == "Patients"
first.half <- data.frame(PTGI_dat[high_exposure_group,
-which(names(PTGI_dat) %in%
c("Groups", "Mean Age" ,
"scale type",
"female proportion(Marg)"))])
second.half <- data.frame(PTGI_dat[!high_exposure_group,
-which(names(PTGI_dat) %in%
c("Groups", "Mean Age" ,
"scale type",
"female proportion(Marg)"))])
new_dat <- cbind(rbind(first.half, second.half),
exposure = ifelse(high_exposure_group == TRUE, 1, 0))
## find out pattern depending on the regions of the world
PTGI_dat$continents
unique(PTGI_dat$continents)
## find out pattern depending on the regions of the world
length(unique(PTGI_dat$continents))
PTGI_dat %>% select(continents == Asia)
PTGI_dat %>% select(continents == 'Asia')
PTGI_dat %>% select(`continents` == 'Asia')
PTGI_dat %>% dplyr::select(continents == 'Asia')
PTGI_dat %>% dplyr::filter(continents == 'Asia')
PTGI_dat %>% dplyr::filter(continents == 'Asia') %>% select(`effect size`)
library(ggplot2)
ggplot(PTGI_dat, aes(x = value, fill = continents)) +
geom_histogram(position = "dodge", alpha = 0.6) +
theme_minimal() +
labs(
title = "Distribution by Continent",
x = "Value",
y = "Count"
)
ggplot(PTGI_dat, aes(x = `effect size`, fill = continents)) +
geom_histogram(position = "dodge", alpha = 0.6) +
theme_minimal() +
labs(
title = "Distribution by Continent",
x = "Value",
y = "Count"
)
# Combined plot (both histogram and density)
ggplot(PTGI_dat, aes(x = value, fill = continents)) +
geom_histogram(aes(y = ..density..), position = "identity", alpha = 0.3) +
geom_density(alpha = 0.4) +
theme_minimal() +
labs(
title = "Distribution by Continent",
x = "Value",
y = "Density"
) +
scale_fill_viridis_d()
# Combined plot (both histogram and density)
ggplot(PTGI_dat, aes(x = `effect size`, fill = continents)) +
geom_histogram(aes(y = ..density..), position = "identity", alpha = 0.3) +
geom_density(alpha = 0.4) +
theme_minimal() +
labs(
title = "Distribution by Continent",
x = "Value",
y = "Density"
) +
scale_fill_viridis_d()
# First, let's examine the data
table(PTGI_dat$continents)  # See how many observations per continent
# Modified plot with explicit binwidth
ggplot(PTGI_dat, aes(x = `effect size`, fill = continents)) +
geom_histogram(aes(y = ..density..),
binwidth = 0.1,  # Adjust this value based on your data range
position = "identity",
alpha = 0.3) +
geom_density(alpha = 0.4, adjust = 1.5) +  # Added adjust parameter for smoother density
theme_minimal() +
labs(
title = "Effect Size Distribution by Continent",
x = "Effect Size",
y = "Density"
) +
scale_fill_viridis_d()
# Remove any groups with insufficient data
PTGI_filtered <- PTGI_dat %>%
group_by(continents) %>%
filter(n() >= 2) %>%
filter(!is.na(`effect size`))
# Create plot with filtered data
ggplot(PTGI_filtered, aes(x = `effect size`, fill = continents)) +
geom_histogram(aes(y = ..density..),
binwidth = 0.2,  # Adjust based on your data range
position = "identity",
alpha = 0.3) +
geom_density(alpha = 0.4) +
theme_minimal() +
labs(
title = "Effect Size Distribution by Continent",
x = "Effect Size",
y = "Density"
) +
scale_fill_viridis_d()
ggplot(PTGI_dat, aes(x = `effect size`, fill = continents)) +
geom_histogram(position = "dodge", alpha = 0.6) +
theme_minimal() +
labs(
title = "Distribution by Continent",
x = "Value",
y = "Count"
)
ggplot(PTGI_dat, aes(x = `effect size`, fill = continents)) +
geom_histogram(position = "dodge",
alpha = 0.6,
binwidth = 5) +  # Increased binwidth to make bars thicker
theme_minimal() +
labs(
title = "Distribution by Continent",
x = "Effect Size",
y = "Count"
) +
scale_fill_viridis_d()
ggplot(PTGI_dat, aes(x = `effect size`, fill = continents, color = continents)) +
geom_histogram(position = "dodge",
alpha = 0.6,
binwidth = 5) +
geom_density(aes(y = after_stat(count) * 5), # Scale density to match histogram height
alpha = 0.4,
linewidth = 1) +  # Make density lines thicker
theme_minimal() +
labs(
title = "Distribution by Continent",
x = "Effect Size",
y = "Count"
) +
scale_fill_viridis_d() +
scale_color_viridis_d() # Match density line colors with fill colors
##
PTGI_dat$continents == 'Oceania'
##
sum(PTGI_dat$continents == 'Oceania')
2938 + 860 + 622 + 1727 + 314 + 1072 + 38 + 114 + 7 + 1473+ 868 + 349 + 16 + 15+ 17
