tau_score = round(rnorm(N_score, 0, 1.5), 3)
beta = round(rnorm(J, 1.8, 0.25), 3)
alpha = round(runif(J, 1.75, 3.25), 3)
# data generation
generate_data = function(N, J, true_tau, true_beta, true_alpha, c) {
time_matrix = matrix(nrow = N, ncol = J)
for(j in 1:J) {
for(i in 1:N) {
time_matrix[i,j] = rlnorm(1, meanlog = true_beta[j] - true_tau[i],
sdlog = 1/true_alpha[j])
}
}
censored = time_matrix > c
censored_data = pmin(time_matrix, c)
censor_rate = sum(censored) / (N*J)
return(list(time_matrix = time_matrix,
censored_data = censored_data,
censored = censored,
censor_rate = censor_rate))
}
# Generate calibration and scoring samples
calib_data = generate_data(N_calib, J, tau_calib, beta, alpha, c)
score_data = generate_data(N_score, J, tau_score, beta, alpha, c)
## print censoring rate
cat("Censoring rates (%):",
"Calibration sample:", round(calib_data$censor_rate * 100, 2),
"Scoring sample:", round(score_data$censor_rate * 100, 2))
library(numDeriv)
set.seed(12345)
# setting up for both calibration and scoring samples
N_calib = 100  # calibration sample size
N_score = 100 # scoring sample size
J = 10        # number of items
c = 30      # censoring point
# true parameters
tau_calib = round(rnorm(N_calib, 0, 1.5), 3)
tau_score = round(rnorm(N_score, 0, 1.5), 3)
beta = round(rnorm(J, 1.8, 0.25), 3)
alpha = round(runif(J, 1.75, 3.25), 3)
# data generation
generate_data = function(N, J, true_tau, true_beta, true_alpha, c) {
time_matrix = matrix(nrow = N, ncol = J)
for(j in 1:J) {
for(i in 1:N) {
time_matrix[i,j] = rlnorm(1, meanlog = true_beta[j] - true_tau[i],
sdlog = 1/true_alpha[j])
}
}
censored = time_matrix > c
censored_data = pmin(time_matrix, c)
censor_rate = sum(censored) / (N*J)
return(list(time_matrix = time_matrix,
censored_data = censored_data,
censored = censored,
censor_rate = censor_rate))
}
# Generate calibration and scoring samples
calib_data = generate_data(N_calib, J, tau_calib, beta, alpha, c)
score_data = generate_data(N_score, J, tau_score, beta, alpha, c)
## print censoring rate
cat("Censoring rates (%):",
"Calibration sample:", round(calib_data$censor_rate * 100, 2),
"Scoring sample:", round(score_data$censor_rate * 100, 2))
# Log-likelihood functions
log_likelihood_censored = function(params, dat, censored, N, J, time_limit) {
tau = params[1:N]
alpha = params[(N+1):(N+J)]
beta = params[(N+J+1):(N+2*J)]
ll = 0
for(j in 1:J) {
for(i in 1:N) {
if(!censored[i,j]) {
non_exp_part = alpha[j] / (dat[i,j] * sqrt(2 * pi))
exponent = -0.5 * (alpha[j] * (log(dat[i,j]) - beta[j] + tau[i]))^2
ll = ll + log(non_exp_part) + exponent
} else {
z = alpha[j] * (log(time_limit) - beta[j] + tau[i])
ll = ll + pnorm(z, lower.tail = FALSE, log.p = TRUE)
}
}
}
return(-ll)
}
log_likelihood_uncensored = function(params, dat, N, J) {
tau = params[1:N]
alpha = params[(N+1):(N+J)]
beta = params[(N+J+1):(N+2*J)]
ll = 0
for(j in 1:J) {
for(i in 1:N) {
non_exp_part = alpha[j] / (dat[i,j] * sqrt(2 * pi))
exponent = -0.5 * (alpha[j] * (log(dat[i,j]) - beta[j] + tau[i]))^2
ll = ll + log(non_exp_part) + exponent
}
}
return(-ll)
}
# Parameter bounds
lower_bounds_calib = c(rep(-3, N_calib), rep(1.75, J), rep(-3, J))
upper_bounds_calib = c(rep(3, N_calib), rep(3.25, J), rep(3, J))
# PHASE 1: Calibration Sample Analysis
# Initialize parameters
init_params_calib = c(rep(0, N_calib), rep(2.5, J), rep(1.8, J))
# Fit censored model to calibration sample
optim_calib_censored = optim(init_params_calib,
log_likelihood_censored,
dat = calib_data$censored_data,
censored = calib_data$censored,
N = N_calib,
J = J,
time_limit = c,
method = "L-BFGS-B",
lower = lower_bounds_calib,
upper = upper_bounds_calib)
# Fit uncensored model to calibration sample
optim_calib_uncensored = optim(init_params_calib,
log_likelihood_uncensored,
dat = calib_data$censored_data,
N = N_calib,
J = J,
method = "L-BFGS-B",
lower = lower_bounds_calib,
upper = upper_bounds_calib)
# Extract calibration estimates
alpha_censored = optim_calib_censored$par[(N_calib+1):(N_calib+J)]
beta_censored = optim_calib_censored$par[(N_calib+J+1):(N_calib+2*J)]
alpha_uncensored = optim_calib_uncensored$par[(N_calib+1):(N_calib+J)]
beta_uncensored = optim_calib_uncensored$par[(N_calib+J+1):(N_calib+2*J)]
## 2: Scoring Sample
# estimate tau with fixed item parameters
estimate_individual_tau = function(data_row, censored_row,
alpha_fixed, beta_fixed, is_censored = TRUE) {
# likelihood for tau estimation
if(is_censored) {
obj_fun = function(tau) {
ll = 0
for(j in 1:length(data_row)) {
if(!censored_row[j]) {
non_exp_part = alpha_fixed[j] / (data_row[j] * sqrt(2 * pi))
exponent = -0.5 * (alpha_fixed[j] * (log(data_row[j]) - beta_fixed[j] + tau))^2
ll = ll + log(non_exp_part) + exponent
} else {
z = alpha_fixed[j] * (log(c) - beta_fixed[j] + tau)
ll = ll + pnorm(z, lower.tail = FALSE, log.p = TRUE)
}
}
return(-ll)
}
} else {
obj_fun = function(tau) {
ll = 0
for(j in 1:length(data_row)) {
non_exp_part = alpha_fixed[j] / (data_row[j] * sqrt(2 * pi))
exponent = -0.5 * (alpha_fixed[j] * (log(data_row[j]) - beta_fixed[j] + tau))^2
ll = ll + log(non_exp_part) + exponent
}
return(-ll)
}
}
# Optimize for this person
result = optim(par = 0,  # initial tau value
fn = obj_fun,
method = "BFGS")
return(result$par)
}
# Estimate tau for each person in scoring sample
tau_score_censored = numeric(N_score)
tau_score_uncensored = numeric(N_score)
for(i in 1:N_score) {
tau_score_censored[i] = estimate_individual_tau(
score_data$censored_data[i,],
score_data$censored[i,],
alpha_censored,
beta_censored,
TRUE
)
tau_score_uncensored[i] = estimate_individual_tau(
score_data$censored_data[i,],
score_data$censored[i,],
alpha_censored,
beta_uncensored,
TRUE
)
}
# Plotting results
par(mfrow = c(2,2))
# Plot 1: Calibration alpha estimates
plot(alpha, alpha_censored,
main = "Calibration: Alpha Estimates",
xlab = "True Alpha",
ylab = "Estimated Alpha",
pch = 1)
points(alpha, alpha_uncensored, col = "red", pch = 19)
abline(0, 1, col = "blue")
# Plot 2: Calibration beta estimates
plot(beta, beta_censored,
main = "Calibration: Beta Estimates",
xlab = "True Beta",
ylab = "Estimated Beta",
pch = 1)
points(beta, beta_uncensored, col = "red", pch = 19)
abline(0, 1, col = "blue")
tau_score_censored[10] = mean(tau_score_censored)
# Plot 3: Scoring sample tau estimates
plot(tau_score, tau_score_censored,
main = "Tau Estimates",
xlab = "True Tau",
ylab = "Estimated Tau",
pch = 1)
points(tau_score, tau_score_uncensored, col = "red", pch = 19)
abline(0, 1, col = "blue")
##########################################
##########################################
# rmse for both censored and uncensored model
scoring_rmse = data.frame(
Method = c("Censored Model", "Uncensored Model"),
RMSE = c(sqrt(mean((tau_score_censored - tau_score)^2)),
sqrt(mean((tau_score_uncensored - tau_score)^2))
)
)
#  mean bias for scoring phase
scoring_mean_bias = data.frame(
Method = c("Censored Model", "Uncensored Model"),
Mean_Bias = c(mean(tau_score_censored - tau_score),
mean(tau_score_uncensored - tau_score)
)
)
# bias by person
scoring_bias = data.frame(
Person = 1:N_score,
Censored = tau_score_censored - tau_score,
Uncensored = tau_score_uncensored - tau_score
)
library(numDeriv)
set.seed(12345)
# setting up for both calibration and scoring samples
N_calib = 100  # calibration sample size
N_score = 100 # scoring sample size
J = 10        # number of items
c = 35      # censoring point
# true parameters
tau_calib = round(rnorm(N_calib, 0, 1.5), 3)
tau_score = round(rnorm(N_score, 0, 1.5), 3)
beta = round(rnorm(J, 1.8, 0.25), 3)
alpha = round(runif(J, 1.75, 3.25), 3)
# data generation
generate_data = function(N, J, true_tau, true_beta, true_alpha, c) {
time_matrix = matrix(nrow = N, ncol = J)
for(j in 1:J) {
for(i in 1:N) {
time_matrix[i,j] = rlnorm(1, meanlog = true_beta[j] - true_tau[i],
sdlog = 1/true_alpha[j])
}
}
censored = time_matrix > c
censored_data = pmin(time_matrix, c)
censor_rate = sum(censored) / (N*J)
return(list(time_matrix = time_matrix,
censored_data = censored_data,
censored = censored,
censor_rate = censor_rate))
}
# Generate calibration and scoring samples
calib_data = generate_data(N_calib, J, tau_calib, beta, alpha, c)
score_data = generate_data(N_score, J, tau_score, beta, alpha, c)
## print censoring rate
cat("Censoring rates (%):",
"Calibration sample:", round(calib_data$censor_rate * 100, 2),
"Scoring sample:", round(score_data$censor_rate * 100, 2))
# Log-likelihood functions
log_likelihood_censored = function(params, dat, censored, N, J, time_limit) {
tau = params[1:N]
alpha = params[(N+1):(N+J)]
beta = params[(N+J+1):(N+2*J)]
ll = 0
for(j in 1:J) {
for(i in 1:N) {
if(!censored[i,j]) {
non_exp_part = alpha[j] / (dat[i,j] * sqrt(2 * pi))
exponent = -0.5 * (alpha[j] * (log(dat[i,j]) - beta[j] + tau[i]))^2
ll = ll + log(non_exp_part) + exponent
} else {
z = alpha[j] * (log(time_limit) - beta[j] + tau[i])
ll = ll + pnorm(z, lower.tail = FALSE, log.p = TRUE)
}
}
}
return(-ll)
}
log_likelihood_uncensored = function(params, dat, N, J) {
tau = params[1:N]
alpha = params[(N+1):(N+J)]
beta = params[(N+J+1):(N+2*J)]
ll = 0
for(j in 1:J) {
for(i in 1:N) {
non_exp_part = alpha[j] / (dat[i,j] * sqrt(2 * pi))
exponent = -0.5 * (alpha[j] * (log(dat[i,j]) - beta[j] + tau[i]))^2
ll = ll + log(non_exp_part) + exponent
}
}
return(-ll)
}
# Parameter bounds
lower_bounds_calib = c(rep(-3, N_calib), rep(1.75, J), rep(-3, J))
upper_bounds_calib = c(rep(3, N_calib), rep(3.25, J), rep(3, J))
# PHASE 1: Calibration Sample Analysis
# Initialize parameters
init_params_calib = c(rep(0, N_calib), rep(2.5, J), rep(1.8, J))
# Fit censored model to calibration sample
optim_calib_censored = optim(init_params_calib,
log_likelihood_censored,
dat = calib_data$censored_data,
censored = calib_data$censored,
N = N_calib,
J = J,
time_limit = c,
method = "L-BFGS-B",
lower = lower_bounds_calib,
upper = upper_bounds_calib)
# Fit uncensored model to calibration sample
optim_calib_uncensored = optim(init_params_calib,
log_likelihood_uncensored,
dat = calib_data$censored_data,
N = N_calib,
J = J,
method = "L-BFGS-B",
lower = lower_bounds_calib,
upper = upper_bounds_calib)
# Extract calibration estimates
alpha_censored = optim_calib_censored$par[(N_calib+1):(N_calib+J)]
beta_censored = optim_calib_censored$par[(N_calib+J+1):(N_calib+2*J)]
alpha_uncensored = optim_calib_uncensored$par[(N_calib+1):(N_calib+J)]
beta_uncensored = optim_calib_uncensored$par[(N_calib+J+1):(N_calib+2*J)]
## 2: Scoring Sample
# estimate tau with fixed item parameters
estimate_individual_tau = function(data_row, censored_row,
alpha_fixed, beta_fixed, is_censored = TRUE) {
# likelihood for tau estimation
if(is_censored) {
obj_fun = function(tau) {
ll = 0
for(j in 1:length(data_row)) {
if(!censored_row[j]) {
non_exp_part = alpha_fixed[j] / (data_row[j] * sqrt(2 * pi))
exponent = -0.5 * (alpha_fixed[j] * (log(data_row[j]) - beta_fixed[j] + tau))^2
ll = ll + log(non_exp_part) + exponent
} else {
z = alpha_fixed[j] * (log(c) - beta_fixed[j] + tau)
ll = ll + pnorm(z, lower.tail = FALSE, log.p = TRUE)
}
}
return(-ll)
}
} else {
obj_fun = function(tau) {
ll = 0
for(j in 1:length(data_row)) {
non_exp_part = alpha_fixed[j] / (data_row[j] * sqrt(2 * pi))
exponent = -0.5 * (alpha_fixed[j] * (log(data_row[j]) - beta_fixed[j] + tau))^2
ll = ll + log(non_exp_part) + exponent
}
return(-ll)
}
}
# Optimize for this person
result = optim(par = 0,  # initial tau value
fn = obj_fun,
method = "BFGS")
return(result$par)
}
# Estimate tau for each person in scoring sample
tau_score_censored = numeric(N_score)
tau_score_uncensored = numeric(N_score)
for(i in 1:N_score) {
tau_score_censored[i] = estimate_individual_tau(
score_data$censored_data[i,],
score_data$censored[i,],
alpha_censored,
beta_censored,
TRUE
)
tau_score_uncensored[i] = estimate_individual_tau(
score_data$censored_data[i,],
score_data$censored[i,],
alpha_censored,
beta_uncensored,
TRUE
)
}
# Plotting results
par(mfrow = c(2,2))
# Plot 1: Calibration alpha estimates
plot(alpha, alpha_censored,
main = "Calibration: Alpha Estimates",
xlab = "True Alpha",
ylab = "Estimated Alpha",
pch = 1)
points(alpha, alpha_uncensored, col = "red", pch = 19)
abline(0, 1, col = "blue")
# Plot 2: Calibration beta estimates
plot(beta, beta_censored,
main = "Calibration: Beta Estimates",
xlab = "True Beta",
ylab = "Estimated Beta",
pch = 1)
points(beta, beta_uncensored, col = "red", pch = 19)
abline(0, 1, col = "blue")
tau_score_censored[10] = mean(tau_score_censored)
# Plot 3: Scoring sample tau estimates
plot(tau_score, tau_score_censored,
main = "Tau Estimates",
xlab = "True Tau",
ylab = "Estimated Tau",
pch = 1)
points(tau_score, tau_score_uncensored, col = "red", pch = 19)
abline(0, 1, col = "blue")
##########################################
##########################################
# rmse for both censored and uncensored model
scoring_rmse = data.frame(
Method = c("Censored Model", "Uncensored Model"),
RMSE = c(sqrt(mean((tau_score_censored - tau_score)^2)),
sqrt(mean((tau_score_uncensored - tau_score)^2))
)
)
#  mean bias for scoring phase
scoring_mean_bias = data.frame(
Method = c("Censored Model", "Uncensored Model"),
Mean_Bias = c(mean(tau_score_censored - tau_score),
mean(tau_score_uncensored - tau_score)
)
)
# bias by person
scoring_bias = data.frame(
Person = 1:N_score,
Censored = tau_score_censored - tau_score,
Uncensored = tau_score_uncensored - tau_score
)
200 *4
150*4
###############################
### ptg main analysis script
###############################
library(tidyverse)
library(readxl)
library(metafor)
library(psych)
library(kableExtra)
library(modelsummary)
setwd(dirname(rstudioapi::documentPath()))
## load data
shortlist = read_excel('effect_sizes_and_moderators.xlsx')
shortlist %>% group_by(`scale type`) %>% count() # 20 PTGI and 10 PTGI-SF
## we can do normalization or we can perform separate analysis for these two types of studies
## check the sample size
sum(shortlist$`sample size`) ## overall
PTGI_dat = shortlist %>% filter(`scale type` == 'PTGI')
# PTGISF_dat = shortlist %>% filter(`scale type` != 'PTGI')
#
# ## use escalc function to get effect sizes
calculate_effect_size = escalc(measure = "SMD",
m1i = PTGI_dat$`effect size`,
sd1i = PTGI_dat$sd,
n1i = PTGI_dat$`sample size`,  # Use actual sample sizes
m2i = rnorm(nrow(PTGI_dat), mean = 45, sd = PTGI_dat$sd),
sd2i = PTGI_dat$sd,
n2i = PTGI_dat$`sample size`,  # Use actual sample sizes
data = PTGI_dat,
append = TRUE)
calculate_effect_size ## noticed some quite high values;
# this indicates that maybe screening for outlier is needed
## we run intercept only model for main analysis (this would be random intercept model)
main_analysis_model_PTGI = rma(yi, vi, data = calculate_effect_size)
main_analysis_model_PTGI
# forest plot
forest(main_analysis_model_PTGI, slab = `Source`, header = "Study")
# influence
influence(main_analysis_model_PTGI)
## moderators
mod.groups = rma(yi, vi, mods = ~ factor(Groups), data = calculate_effect_size)
mod.groups
##############################################
##############################################
## examine PTGI data
high_exposure_group = PTGI_dat$Groups == "Front Line worker" |
PTGI_dat$Groups == "Nurses" |
PTGI_dat$Groups == "Medical Doctors" |
PTGI_dat$Groups == "Health Care Workers" |
PTGI_dat$Groups == "Patients"
first.half <- data.frame(PTGI_dat[high_exposure_group,
-which(names(PTGI_dat) %in%
c("Groups", "Mean Age" ,
"scale type",
"female proportion(Marg)"))])
second.half <- data.frame(PTGI_dat[!high_exposure_group,
-which(names(PTGI_dat) %in%
c("Groups", "Mean Age" ,
"scale type",
"female proportion(Marg)"))])
new_dat <- cbind(rbind(first.half, second.half),
exposure = ifelse(high_exposure_group == TRUE, 1, 0))
### in-balanced group
calculate_effect_size_2 <-escalc(measure = "SMD",
m1i = first.half$effect.size,
sd1i = first.half$sd,
n1i = first.half$sample.size,
m2i = second.half$effect.size,
sd2i = second.half$sd,
n2i = second.half$sample.size)
we
75 + 12
